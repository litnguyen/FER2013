{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from data.fer2013 import get_dataloaders\n",
    "from utils.checkpoint import save\n",
    "from utils.hparams import setup_hparams\n",
    "from utils.loops import train, evaluate\n",
    "from utils.setup_network import setup_network\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def run(net, logger, hps):\n",
    "    # Create dataloaders\n",
    "    trainloader, valloader, testloader = get_dataloaders(bs=hps['bs'])\n",
    "\n",
    "    net = net.to(device)\n",
    "\n",
    "    learning_rate = float(hps['lr'])\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # optimizer = torch.optim.Adadelta(net.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    # optimizer = torch.optim.Adagrad(net.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0001, amsgrad=True)\n",
    "    # optimizer = torch.optim.ASGD(net.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 20, gamma=0.5, last_epoch=-1, verbose=True)\n",
    "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(trainloader), epochs=hps['n_epochs'])\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1, verbose=True)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6, last_epoch=-1, verbose=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    print(\"Training\", hps['name'], \"on\", device)\n",
    "    for epoch in range(hps['start_epoch'], hps['n_epochs']):\n",
    "\n",
    "        acc_tr, loss_tr = train(net, trainloader, criterion, optimizer, scaler)\n",
    "        logger.loss_train.append(loss_tr)\n",
    "        logger.acc_train.append(acc_tr)\n",
    "\n",
    "        acc_v, loss_v = evaluate(net, valloader, criterion)\n",
    "        logger.loss_val.append(loss_v)\n",
    "        logger.acc_val.append(acc_v)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(acc_v)\n",
    "\n",
    "        if acc_v > best_acc and (epoch+1) >= 50:\n",
    "            best_acc = acc_v\n",
    "\n",
    "            save(net, logger, hps, epoch + 1)\n",
    "            logger.save_plt(hps)\n",
    "\n",
    "        if (epoch + 1) % hps['save_freq'] == 0:\n",
    "            save(net, logger, hps, epoch + 1)\n",
    "            logger.save_plt(hps)\n",
    "\n",
    "        print('Epoch %2d' % (epoch + 1),\n",
    "              'Train Accuracy: %2.4f %%' % acc_tr,\n",
    "              'Val Accuracy: %2.4f %%' % acc_v,\n",
    "              sep='\\t\\t')\n",
    "\n",
    "    # Calculate performance on test set\n",
    "    acc_test, loss_test = evaluate(net, testloader, criterion)\n",
    "    print('Test Accuracy: %2.4f %%' % acc_test,\n",
    "          'Test Loss: %2.6f' % loss_test,\n",
    "          sep='\\t\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--ip=127.0.0.1',\n",
       " '--stdin=9003',\n",
       " '--control=9001',\n",
       " '--hb=9000',\n",
       " '--Session.signature_scheme=\"hmac-sha256\"',\n",
       " '--Session.key=b\"bc9e174e-feed-49ef-b787-74149cf2fa68\"',\n",
       " '--shell=9002',\n",
       " '--transport=\"tcp\"',\n",
       " '--iopub=9004',\n",
       " '--f=\"c:\\\\Users\\\\Chi Khang\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-9152LjfjqbGLAzFV.json\"']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = setup_hparams()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chikhang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55b8e25920f24e5fb540a5617ed9e83a094b314ca164b2901879d0fca1c9fbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
